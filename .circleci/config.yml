# WARNING: DO NOT EDIT THIS FILE DIRECTLY!!!
# See the README.md in this directory.

# IMPORTANT: To update Docker image version, please first update
# https://github.com/pytorch/ossci-job-dsl/blob/master/src/main/groovy/ossci/pytorch/DockerVersion.groovy and
# https://github.com/pytorch/ossci-job-dsl/blob/master/src/main/groovy/ossci/caffe2/DockerVersion.groovy,
# and then update DOCKER_IMAGE_VERSION at the top of the following files:
# * cimodel/data/pytorch_build_definitions.py
# * cimodel/data/caffe2_build_definitions.py
# And the inline copies of the variable in
# * verbatim-sources/job-specs-custom.yml
#   (grep for DOCKER_IMAGE)

version: 2.1

docker_config_defaults: &docker_config_defaults
  user: jenkins
  aws_auth:
    # This IAM user only allows read-write access to ECR
    aws_access_key_id: ${CIRCLECI_AWS_ACCESS_KEY_FOR_ECR_READ_WRITE_V4}
    aws_secret_access_key: ${CIRCLECI_AWS_SECRET_KEY_FOR_ECR_READ_WRITE_V4}

# This system setup script is meant to run before the CI-related scripts, e.g.,
# installing Git client, checking out code, setting up CI env, and
# building/testing.
setup_linux_system_environment: &setup_linux_system_environment
  name: Set Up System Environment
  no_output_timeout: "1h"
  command: ~/workspace/.circleci/scripts/setup_linux_system_environment.sh

# NB: This (and the command below) must be run after attaching
# ~/workspace.  This is NOT the default working directory (that's
# ~/project); this workspace is generated by the setup job.
should_run_job: &should_run_job
  name: Should Run Job After attach_workspace
  no_output_timeout: "2m"
  command: ~/workspace/.circleci/scripts/should_run_job.sh

setup_ci_environment: &setup_ci_environment
  name: Set Up CI Environment After attach_workspace
  no_output_timeout: "1h"
  command: ~/workspace/.circleci/scripts/setup_ci_environment.sh

# Installs expect and moreutils so that we can call `unbuffer` and `ts`.
# Also installs OpenMP
# !!!!NOTE!!!! this is copied into a binary_macos_brew_update job which is the
# same but does not install libomp. If you are changing this, consider if you
# need to change that step as well.
macos_brew_update: &macos_brew_update
  name: Brew update and install moreutils, expect and libomp
  no_output_timeout: "1h"
  command: |
    set -ex
    # See https://discourse.brew.sh/t/fetching-homebrew-repos-is-slow/5374/3
    brew untap caskroom/homebrew-cask
    # moreutils installs a `parallel` executable by default, which conflicts
    # with the executable from the GNU `parallel`, so we must unlink GNU
    # `parallel` first, and relink it afterwards
    brew update
    brew unlink parallel
    brew install moreutils
    brew link parallel --overwrite
    brew install expect
    brew install libomp

ios_brew_update: &ios_brew_update
  name: Brew update and install iOS toolchains
  no_output_timeout: "1h"
  command: |
    set -ex
    brew update
    brew unlink parallel
    brew install moreutils
    brew link parallel --overwrite
    brew install expect
    brew install libtool

    ##############################################################################
# Build parameters
##############################################################################
pytorch_params: &pytorch_params
  parameters:
    build_environment:
      type: string
      default: ""
    docker_image:
      type: string
      default: ""
    resource_class:
      type: string
      default: "large"
    use_cuda_docker_runtime:
      type: string
      default: ""
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    DOCKER_IMAGE: << parameters.docker_image >>
    USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
  resource_class: << parameters.resource_class >>

pytorch_ios_params: &pytorch_ios_params
  parameters:
    build_environment:
      type: string
      default: ""
    ios_arch:
      type: string
      default: ""
    ios_platform:
      type: string
      default: ""
    use_nnpack:
      type: string
      default: "ON"
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    IOS_ARCH: << parameters.ios_arch >>
    IOS_PLATFORM: << parameters.ios_platform >>
    USE_NNPACK: << parameters.use_nnpack >>



binary_linux_build_params: &binary_linux_build_params
  parameters:
    build_environment:
      type: string
      default: ""
    docker_image:
      type: string
      default: ""
    libtorch_variant:
      type: string
      default: ""
    resource_class:
      type: string
      default: "2xlarge+"
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    LIBTORCH_VARIANT: << parameters.libtorch_variant >>
  resource_class: << parameters.resource_class >>
  docker:
    - image: << parameters.docker_image >>

binary_linux_test_upload_params: &binary_linux_test_upload_params
  parameters:
    build_environment:
      type: string
      default: ""
    docker_image:
      type: string
      default: ""
    libtorch_variant:
      type: string
      default: ""
    resource_class:
      type: string
      default: "medium"
    use_cuda_docker_runtime:
      type: string
      default: ""
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    DOCKER_IMAGE: << parameters.docker_image >>
    USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
    LIBTORCH_VARIANT: << parameters.libtorch_variant >>
  resource_class: << parameters.resource_class >>

binary_mac_params: &binary_mac_params
  parameters:
    build_environment:
      type: string
      default: ""
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>

##############################################################################
# Job specs
##############################################################################
jobs:
  pytorch_linux_build:
    <<: *pytorch_params
    machine:
      image: ubuntu-1604:201903-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - run:
        <<: *setup_linux_system_environment
    - checkout
    - run:
        <<: *setup_ci_environment
    - run:
        name: Build
        no_output_timeout: "1h"
        command: |
          set -e
          # Pull Docker image and run build
          echo "DOCKER_IMAGE: "${DOCKER_IMAGE}
          docker pull ${DOCKER_IMAGE} >/dev/null
          export id=$(docker run -t -d -w /var/lib/jenkins ${DOCKER_IMAGE})

          git submodule sync && git submodule update -q --init --recursive

          docker cp /home/circleci/project/. $id:/var/lib/jenkins/workspace

          if [[ ${BUILD_ENVIRONMENT} == *"namedtensor"* ]]; then
            NAMED_FLAG="export BUILD_NAMEDTENSOR=1"
          fi

          export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo '"$NAMED_FLAG"' && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/build.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          # Push intermediate Docker image for next phase to use
          if [ -z "${BUILD_ONLY}" ]; then
            # Note [Special build images]
            # The namedtensor and xla builds use the same docker image as
            # pytorch-linux-trusty-py3.6-gcc5.4-build. In the push step, we have to
            # distinguish between them so the test can pick up the correct image.
            output_image=${DOCKER_IMAGE}-${CIRCLE_SHA1}
            if [[ ${BUILD_ENVIRONMENT} == *"namedtensor"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-namedtensor
            elif [[ ${BUILD_ENVIRONMENT} == *"xla"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-xla
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-x86_64"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-x86_64
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-arm-v7a"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-arm-v7a
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-arm-v8a"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-arm-v8a
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-x86_32"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-x86_32
            else
              export COMMIT_DOCKER_IMAGE=$output_image
            fi
            docker commit "$id" ${COMMIT_DOCKER_IMAGE}
            docker push ${COMMIT_DOCKER_IMAGE}
          fi

  pytorch_linux_test:
    <<: *pytorch_params
    machine:
      image: ubuntu-1604:201903-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - run:
        <<: *setup_linux_system_environment
    - run:
        <<: *setup_ci_environment
    - run:
        name: Test
        no_output_timeout: "90m"
        command: |
          set -e
          # See Note [Special build images]
          output_image=${DOCKER_IMAGE}-${CIRCLE_SHA1}
          if [[ ${BUILD_ENVIRONMENT} == *"namedtensor"* ]]; then
            export COMMIT_DOCKER_IMAGE=$output_image-namedtensor
            NAMED_FLAG="export BUILD_NAMEDTENSOR=1"
          elif [[ ${BUILD_ENVIRONMENT} == *"xla"* ]]; then
            export COMMIT_DOCKER_IMAGE=$output_image-xla
          else
            export COMMIT_DOCKER_IMAGE=$output_image
          fi
          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}
          docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null
          if [ -n "${USE_CUDA_DOCKER_RUNTIME}" ]; then
            export id=$(docker run --runtime=nvidia -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          else
            export id=$(docker run -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          fi
          if [[ ${BUILD_ENVIRONMENT} == *"multigpu"* ]]; then
            export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo '"$NAMED_FLAG"' && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/multigpu-test.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          else
            export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo '"$NAMED_FLAG"'&& echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/test.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          fi
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts
  binary_linux_build:
    <<: *binary_linux_build_params
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Install unbuffer and ts
        command: |
            set -eux -o pipefail
            source /env
            OS_NAME=`awk -F= '/^NAME/{print $2}' /etc/os-release`
            if [[ "$OS_NAME" == *"CentOS Linux"* ]]; then
              retry yum -q -y install epel-release
              retry yum -q -y install expect moreutils
            elif [[ "$OS_NAME" == *"Ubuntu"* ]]; then
              retry apt-get update
              retry apt-get -y install expect moreutils
              conda install -y -c eumetsat expect
              conda install -y cmake
            fi
    - run:
        name: Update compiler to devtoolset7
        command: |
            set -eux -o pipefail
            source /env
            if [[ "$DESIRED_DEVTOOLSET" == 'devtoolset7' ]]; then
              source "/builder/update_compiler.sh"

              # Env variables are not persisted into the next step
              echo "export PATH=$PATH" >> /env
              echo "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH" >> /env
            else
              echo "Not updating compiler"
            fi
    - run:
        name: Build
        no_output_timeout: "1h"
        command: |
            source "/pytorch/.circleci/scripts/binary_linux_build.sh"
    - persist_to_workspace:
        root: /
        paths: final_pkgs

    # This should really just be another step of the binary_linux_build job above.
    # This isn't possible right now b/c the build job uses the docker executor
    # (otherwise they'd be really really slow) but this one uses the macine
    # executor (b/c we have to run the docker with --runtime=nvidia and we can't do
    # that on the docker executor)
  binary_linux_test:
    <<: *binary_linux_test_upload_params
    machine:
        image: ubuntu-1604:201903-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    # TODO: We shouldn't attach the workspace multiple times
    - attach_workspace:
        at: /home/circleci/project
    - run:
        <<: *should_run_job
    - run:
        <<: *setup_linux_system_environment
    - run:
        <<: *setup_ci_environment
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Prepare test code
        no_output_timeout: "1h"
        command: ~/workspace/.circleci/scripts/binary_linux_test.sh
    - run:
        <<: *binary_run_in_docker

  binary_linux_upload:
    <<: *binary_linux_test_upload_params
    machine:
        image: ubuntu-1604:201903-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - run:
        <<: *setup_linux_system_environment
    - run:
        <<: *setup_ci_environment
    - attach_workspace:
        at: /home/circleci/project
    - run:
        <<: *binary_populate_env
    - run:
        <<: *binary_install_miniconda
    - run:
        name: Upload
        no_output_timeout: "1h"
        command: ~/workspace/.circleci/scripts/binary_linux_upload.sh

  # Nighlty build smoke tests defaults
  # These are the second-round smoke tests. These make sure that the binaries are
  # correct from a user perspective, testing that they exist from the cloud are
  # are runnable. Note that the pytorch repo is never cloned into these jobs
  ##############################################################################
  smoke_linux_test:
    <<: *binary_linux_test_upload_params
    machine:
      image: ubuntu-1604:201903-01
    steps:
    - attach_workspace:
        at: ~/workspace
    - attach_workspace:
        at: /home/circleci/project
    - run:
        <<: *setup_linux_system_environment
    - run:
        <<: *setup_ci_environment
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Test
        no_output_timeout: "1h"
        command: |
          set -ex
          cat >/home/circleci/project/ci_test_script.sh \<<EOL
          # The following code will be executed inside Docker container
          set -eux -o pipefail
          /builder/smoke_test.sh
          # The above code will be executed inside Docker container
          EOL
    - run:
        <<: *binary_run_in_docker

  smoke_mac_test:
    <<: *binary_linux_test_upload_params
    macos:
      xcode: "9.0"
    steps:
      - attach_workspace:
          at: ~/workspace
      - attach_workspace: # TODO - we can `cp` from ~/workspace
          at: /Users/distiller/project
      - run:
          <<: *binary_checkout
      - run:
          <<: *binary_populate_env
      - run:
          <<: *binary_macos_brew_update
      - run:
          <<: *binary_install_miniconda
      - run:
          name: Build
          no_output_timeout: "1h"
          command: |
            set -ex
            source "/Users/distiller/project/env"
            export "PATH=$workdir/miniconda/bin:$PATH"
            # TODO unbuffer and ts this, but it breaks cause miniconda overwrites
            # tclsh. But unbuffer and ts aren't that important so they're just
            # disabled for now
            ./builder/smoke_test.sh

  binary_mac_build:
    <<: *binary_mac_params
    macos:
      xcode: "9.0"
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        <<: *binary_macos_brew_update
    - run:
        <<: *binary_install_miniconda

    - run:
        name: Build
        no_output_timeout: "1h"
        command: |
          set -eux -o pipefail
          script="/Users/distiller/project/pytorch/.circleci/scripts/binary_macos_build.sh"
          cat "$script"
          source "$script"

    - run:
        name: Test
        no_output_timeout: "1h"
        command: |
          set -eux -o pipefail
          script="/Users/distiller/project/pytorch/.circleci/scripts/binary_macos_test.sh"
          cat "$script"
          source "$script"

    - persist_to_workspace:
        root: /Users/distiller/project
        paths: final_pkgs

  binary_mac_upload: &binary_mac_upload
    <<: *binary_mac_params
    macos:
      xcode: "9.0"
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        <<: *binary_macos_brew_update
    - run:
        <<: *binary_install_miniconda
    - attach_workspace: # TODO - we can `cp` from ~/workspace
        at: /Users/distiller/project
    - run:
        name: Upload
        no_output_timeout: "10m"
        command: |
          script="/Users/distiller/project/pytorch/.circleci/scripts/binary_macos_upload.sh"
          cat "$script"
          source "$script"

  binary_ios_build:
    <<: *pytorch_ios_params
    macos:
      xcode: "10.2.1"
    steps:
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - checkout
    - run:
        <<: *ios_brew_update
    - run:
        name: Build
        contxt: org-member
        no_output_timeout: "1h"
        command: |
          set -eux -o pipefail
          script="/Users/distiller/project/.circleci/scripts/binary_ios_build.sh"
          cat "$script"
          source "$script"
    - persist_to_workspace:
        root: /Users/distiller/workspace/
        paths: ios
  
  binary_ios_upload: 
    <<: *pytorch_ios_params
    macos:
      xcode: "10.2.1"
    steps:
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - checkout
    - run:
        <<: *ios_brew_update
    - run:
        name: Upload
        no_output_timeout: "1h"
        command: |
          script="/Users/distiller/project/.circleci/scripts/binary_ios_upload.sh"
          cat "$script"
          source "$script"  
  setup:
    docker:
      - image: circleci/python:3.7.3
    steps:
      - checkout
      - run:
          name: Ensure config is up to date
          command: ./ensure-consistency.py
          working_directory: .circleci
      - run:
          name: Save commit message
          command: git log --format='%B' -n 1 HEAD > .circleci/scripts/COMMIT_MSG
      # Note [Workspace for CircleCI scripts]
      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      # In the beginning, you wrote your CI scripts in a
      # .circleci/config.yml file, and life was good.  Your CI
      # configurations flourished and multiplied.
      #
      # Then one day, CircleCI cometh down high and say, "Your YAML file
      # is too biggeth, it stresses our servers so."  And thus they
      # asketh us to smite the scripts in the yml file.
      #
      # But you can't just put the scripts in the .circleci folder,
      # because in some jobs, you don't ever actually checkout the
      # source repository.  Where you gonna get the scripts from?
      #
      # Here's how you do it: you persist .circleci/scripts into a
      # workspace, attach the workspace in your subjobs, and run all
      # your scripts from there.
      - persist_to_workspace:
          root: .
          paths: .circleci/scripts


##############################################################################
##############################################################################
# Workflows
##############################################################################
##############################################################################

# PR jobs pr builds
workflows:
  build:
    jobs:
      - setup
      # Pytorch iOS builds
      - binary_ios_build:
          name: pytorch_ios_10_2_1_nightly_x86_64_build
          build_environment: "libtorch-ios-10.2.1-nightly-x86_64-build"
          ios_platform: "SIMULATOR"
          ios_arch: "x86_64"
          use_nnpack: "OFF"
          requires: 
            - setup
      - binary_ios_build:
          name: pytorch_ios_10_2_1_nigthly_arm64_build
          build_environment: "libtorch-ios-10.2.1-nightly-arm64-build"
          ios_arch: "arm64"
          ios_platform: "OS"
          requires: 
            - setup
      - binary_ios_build:
          name: pytorch_ios_10_2_1_nightly_armv7s_build
          build_environment: "libtorch-ios-10.2.1-nightly-armv7s-build"
          ios_arch: "armv7s"
          ios_platform: "OS"
          requires: 
            - setup
      - binary_ios_upload:
          build_environment: "libtorch-ios-bianry-upload"
          context: org-member
          requires:
            - setup
            - pytorch_ios_10_2_1_nightly_x86_64_build
            - pytorch_ios_10_2_1_nigthly_arm64_build
            - pytorch_ios_10_2_1_nightly_armv7s_build